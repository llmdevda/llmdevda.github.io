---
title: "OpenR1实战(3)--SFT训练"
date: 2025-05-21
categories: ["实战", "教程"]
tags: [llm, grpo, openr1]
---
上节完成了数据准备

接下来可以修改recipes菜单里的配置项来训练我们的数据集

因为我们用的模型支持对话，SFTTrainer也会提取Messages作为训练的input_ids

只要保证有messages这一列就好

通常来说，只要数据集的格式一致就没问题

但最好搞清菜单中配置项的每一项的意义

这样在出现错误时能更快关联问题

比如gradient_checkpointing，它不能与kvcache混用，因此会在与kvcache相关的peft中报错。

现在因为GPT的发展，查询配置项的意义功能非常方便

但是检查错误还是有些困难

因此不能逃避责任，还是得踏踏实实的理清每一项的作用

SFTConfig的实际配置在trl库中
```python
@dataclass
class SFTConfig(TrainingArguments):
```

详细含义都可以在代码Doc中找到






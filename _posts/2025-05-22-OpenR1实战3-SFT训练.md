---
title: "OpenR1实战(3)--SFT训练"
date: 2025-05-21
categories: ["实战", "教程"]
tags: [llm, grpo, openr1]
---
上节完成了数据准备

接下来可以修改recipes菜单里的配置项来训练我们的数据集

之前有提过，SFT的过程就是模型模仿写作的过程。

所以理论上只需要一列文本数据就可以

但有一堆隐藏的处理规则

```py
```



简单整理逻辑如下
```mermaid
flowchart TD
    A[输入数据集] --> B{已预处理或特殊类型?}
    B -->|ConstantLengthDataset\n或含input_ids| Z[直接返回]
    B -->|否| C[应用格式化函数\生成text列]
    C --> D[转换为ChatML格式]
    D --> E{对话式数据?}
    E -->|是| F[应用对话模板\(add_special_tokens=False)]
    E -->|否| G[添加EOS标记\n(add_special_tokens=True)]
    F & G --> H[分词处理]
    H --> I{需要打包或截断?}
    I -->|打包| J[按max_length打包]
    I -->|截断| K[按max_length截断]
    J & K --> L{使用Liger内核?}
    L -->|是| M[仅保留input_ids]
    L -->|否| N[保留所有列]
    M & N --> Z
    Z[返回最终数据集]
```

因为我们用的模型支持对话，SFTTrainer也会提取Messages作为训练的input_ids

只要保证有messages这一列就好

通常来说，只要数据集的格式一致就没问题

但最好搞清菜单中配置项的每一项的意义

这样在出现错误时能更快关联问题

比如gradient_checkpointing，它不能与kvcache混用，因此会在与kvcache相关的peft中报错。

现在因为GPT的发展，查询配置项的意义功能非常方便

但是检查错误还是有些困难

因此不能逃避责任，还是得踏踏实实的理清每一项的作用

SFTConfig的实际配置在trl库中
```python
@dataclass
class SFTConfig(TrainingArguments):
```

详细含义都可以在代码Doc中找到


与下一节的GRPO过程不同，SFT是偏向计算消耗型。

因为所有的token都在同一时间给了模型。

而GRPO训练时需要对token进行挨个
如果有






---
title: "OpenR1实战(4)--GRPO训练"
date: 2025-05-22
categories: ["实战", "教程"]
tags: [llm, grpo, openr1]
---
上一节对模型进行SFT后，可以进行GRPO训练了。

在GRPO前进行SFT，是为了保证生成结果基本正确。

GRPO的生成量，适合为正确率的倒数，确保每个批次都能有正确答案。

比如SFT后的模型，如果生成答案的正确率为1/5，那么num_generation就适合设置为5。


```py
class GRPOConfig(TrainingArguments):
```

### 奖励函数


### 推理耗时
主要时间都在LLM推理中，如果是
### loss_type
loss_type和论文中的实现不完全相同，
https://github.com/huggingface/open-r1/issues/239#issuecomment-2646297851
带|oi|的，




temperature 问题与per_token_logps
我不确定论文里有没有提到训练时的temperature，可能需要注意。

这个temperature不是生成时候的，

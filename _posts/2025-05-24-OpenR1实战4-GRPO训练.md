---
title: "OpenR1实战(4)--GRPO训练"
date: 2025-05-24
categories: ["实战", "教程"]
tags: [llm, grpo, openr1]
---
上一节对模型进行SFT后，可以进行GRPO训练了。

在GRPO前进行SFT，是为了保证生成结果基本正确。

而在GRPO过程中，则通过不断的生成与调整，让模型更接近我们的目标函数。

GRPO的配置定义位于下面这个类中
```py
class GRPOConfig(TrainingArguments):
```

我们简要介绍一下重要的项目

### 奖励函数
在yaml文件里配置使用的奖励函数与权重
```yaml
reward_funcs:
- poem_format
- poem_yun
reward_weights:
- 1.0
- 1.0
```
如果是要新加自定义的奖励函数，需要在openr1的REWARD_FUNCS_REGISTRY将其注册进去。
```py
    REWARD_FUNCS_REGISTRY = {
        "poem_format": poem_reward_format,
        "poem_yun": poem_reward_yun,
```

### num_generations
```yaml
num_generations: 4
```
num_generations 也就是GRPO公式中的G，对应与生成G个答案。

$$
\mathcal{J}_{\text{GRPO}}(\theta) = \mathbb{E}[q \sim P(Q),\, \{o_i\}_{i=1}^{G} \sim \pi_{\theta_{\text{old}}}(\mathcal{O}|q)] \\ 
\frac{1}{G} \sum_{i=1}^{G} \left( 
\min\left( 
\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)} A_i,\ 
\operatorname{clip}\left( 
\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)},\ 1 - \varepsilon,\ 1 + \varepsilon 
\right) A_i 

\right) 
- \beta \mathbb{D}_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}}) 
\right) 
$$

**GRPO的生成量，适合为正确率的倒数，确保每个批次都能有正确答案。**

比如SFT后的模型，如果生成答案的正确率为1/5，那么num_generation就适合设置为5。

### num_iterations
```yaml
num_iterations: 1
```
而num_iterations不存在公式中，它是一个内部迭代，起到加速训练的作用。

![](/assets/images/2025-05-24-OpenR1实战4-GRPO训练-num_iterations的算法示意图.png)

因为在GRPO num_iterations的内部迭代中，使用的训练样本是一样的，不需要进行额外的generation。而GRPO的一大耗时瓶颈就在于生成训练样本的推理过程，因此增大num_iterations是不会影响速度的。但是不清楚其实际提升有多少，也不知道其值适合配置多少，openr1中默认配置为1了。


### use_vllm
```yaml
use_vllm: false
```
主要时间都在LLM推理中，如果是在vllm中推理可以大幅加速训练进程。

注意这里的vllm不是直接的vllm server，而是trl封装的一个代理工具server。

其目的是为了通过Nvidia GPU的NCCL协议来动态更新模型参数。

不需要额外加速时，可以先设置为false

### beta
```yaml
beta: 0.03
```
beta来自于GRPO公式中的散度KL的权重

$$
 \beta \mathbb{D}_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}}) 
$$

其用于控制模型训练与参考模型的偏差不会特别大。

在没有指定具体参考模型时，参考模型使用的是原始模型算的（disable_adapter）。

但要注意**KL计算会额外产生新的outputs，当产生的CoT很长时，可能导致显存OOM**。

因为KL主要用于控制与参考模型的偏差，而做lora之类的peft时，本身与原始模型的偏差有限，**因此可以将beta设置为0来减少显存占用**


### loss_type
loss_type和论文中的实现不完全相同，[github的trl作者介绍](https://github.com/huggingface/open-r1/issues/239#issuecomment-2646297851)，其公式是包含|oi|的。

![](/assets/images/2025-05-24-OpenR1实战4-GRPO训练-loss实现与公式对比.png)

**包含|oi|作为倒数的loss，会倾向于低CoT长度的回答。**

我当时没有注意到loss_type的区别，下面的结果是以bnpo进行的

下一篇文章我会详细讲一下loss_type，以及不同loss_type的实验结果


### temperature
temperature会影响两个部分

第一个很容易想到，就是产生样本是的temperature，这个temperature不能太低，否则样本都一个样了，就没有意义了。

**另一个部分是涉及loss的计算，在per_token_logps函数中**

```py
logits = logits / self.temperature
```

我不确定论文里有没有提到loss计算时的temperature，可能需要注意。

**因为这里的self.temperature会导致概率的计算与预想的不一样。**


# 结果
我分别对包含CoT和不包含CoT的模型进行了训练

随着训练次数增加，整体的得分都在上升

![](/assets/images/2025-05-24-OpenR1实战4-GRPO训练-整体奖励的对比.png)

（CoT的部分因为OOM丢失了前600 steps的数据）

![](/assets/images/2025-05-24-OpenR1实战4-GRPO训练-韵的得分.png)
![](/assets/images/2025-05-24-OpenR1实战4-GRPO训练-平仄的得分.png)

**整体收敛的比较接近，因为有SFT打底，整体效果不差**。结果中的提升应该主要都是奖励函数的作用，**SFT后的模型初始得分就比较偏高，GRPO进一步提升了结果**。

**此外CoT模型的训练，平均长度反而有所下降**，这里分析我会放在下一个讲loss type的文章。

![](/assets/images/2025-05-24-OpenR1实战4-GRPO训练-CoT长度下降.png)

**没有CoT的模型，训练要远快于CoT的，因为没有CoT时，生成速度更快。**
![](/assets/images/2025-05-24-OpenR1实战4-GRPO训练-训练时间差异.png)

## 实测
相比抽象的reward分数，我们直接看看对诗效果

第一首是李白的 《东鲁门泛舟》，没有放入训练集的一首诗

> 水作青龙盘石堤，桃花夹岸鲁门西。 
> 
> 若教月下乘舟去，何啻风流到剡溪。



```log
------Q: 水作青龙盘石堤，桃花夹岸鲁门西。若教月下乘舟去，-----------------
原始模型:
 水作青龙盘石堤，桃花夹岸鲁门西。
 若教月下乘舟去，梦回故里花如雪。
【奖励评价】:0.42857142857142855 0.0 七言绝句(仄起平收式) 八齐韵 第22字 [回] 不符合仄,第24字 [里] 不符合平,第26字 [如] 不符合仄,第27字 [雪] 不符合平韵

SFT-无CoT:
 水作青龙盘石堤，桃花夹岸鲁门西。
 若教月下乘舟去，应是仙人放鹤归。
【奖励评价】:1.0 0.0 七言绝句(仄起平收式) 八齐韵 第27字 [归] 不符合平韵

SFT-有CoT:
 水作青龙盘石堤，桃花夹岸鲁门西。
 若教月下乘舟去，应有仙人吹笛啼。
【奖励评价】:0.8571428571428571 1.0 七言绝句(仄起平收式) 八齐韵 第26字 [笛] 不符合仄

GRPO-无CoT:
 水作青龙盘石堤，桃花夹岸鲁门西。
 若教月下乘舟去，一棹烟波去未迷。
【奖励评价】:1.0 1.0 七言绝句(仄起平收式) 八齐韵 

GRPO-有CoT:
 水作青龙盘石堤，桃花夹岸鲁门西。
 若教月下乘舟去，应是仙家梦里迷。
【奖励评价】:1.0 1.0 七言绝句(仄起平收式) 八齐韵 

```

第二首是一个麻将打油诗，作者佚名

> 四人摸打一副牌，东西南北中发白。
> 
> 吃岔杠听清一色，滚滚财源梦里来。

```log
------Q: 四人摸打一副牌，东西南北中发白。吃岔杠听清一色，-----------------
原始模型:
 四人摸打一副牌，东西南北中发白。
 吃岔杠听清一色，胜负分明各归家。
【奖励评价】:0.8571428571428571 1.0 七言绝句(仄起平收式) 六麻韵 第26字 [归] 不符合仄

SFT-无CoT:
 四人摸打一副牌，东西南北中发白。
 吃岔杠听清一色，不与人争先得。
【奖励评价】:0.8571428571428571 0.0 七言绝句(仄起平收式) 九佳韵 第27字 [None] 不符合平韵

SFT-有CoT:
 四人摸打一副牌，东西南北中发白。
 吃岔杠听清一色，胜负分明红白色。
【奖励评价】:0.7142857142857143 0.0 七言绝句(仄起平收式) 九佳韵 第26字 [白] 不符合仄,第27字 [色] 不符合平韵

GRPO-无CoT:
 四人摸打一副牌，东西南北中发白。
 吃岔杠听清一色，红梅映雪映心间。
【奖励评价】:1.0 1.0 七言绝句(平起仄收式) 十五删韵 

GRPO-有CoT:
 四人摸打一副牌，东西南北中发白。
 吃岔杠听清一色，胜负高低皆入画。
【奖励评价】:0.8571428571428571 0.0 七言绝句(仄起平收式) 九佳韵 第27字 [画] 不符合平韵
```

可见这类打油诗诗，与训练集相差比较大，模型表现就很一般了。

而GRPO后的结果，虽然平仄和韵更符合了，但是打油的意思差了些味



第三首是我工作第一年写的诗，《坐火车发现十月七日假期最后一天正好是重阳节好巧啊有感》

> 礼乐飞花迎典庆，青枫白果待秋迟。
> 
> 十一归去逢重九，正是少年登高时。

```log
------Q: 礼乐飞花迎典庆，青枫白果待秋迟。十一归去逢重九，-----------------
原始模型:
 礼乐飞花迎典庆，青枫白果待秋迟。
 十一归去逢重九，醉卧松风。
【奖励评价】:0.5714285714285714 0.0 七言绝句(仄起仄收式) 四支韵 第25字 [None] 不符合(仄),第26字 [None] 不符合仄,第27字 [None] 不符合平韵

SFT-无CoT:
 礼乐飞花迎典庆，青枫白果待秋迟。
 十一归去逢重九，九节秋风送客时。
【奖励评价】:0.8571428571428571 1.0 七言绝句(仄起仄收式) 四支韵 第22字 [节] 不符合仄

SFT-有CoT:
 礼乐飞花迎典庆，青枫白果待秋迟。
 十一归去逢重九，重阳佳节又相期。
【奖励评价】:0.8571428571428571 1.0 七言绝句(仄起仄收式) 四支韵 第22字 [阳] 不符合仄

GRPO-无CoT:
 礼乐飞花迎典庆，青枫白果待秋迟。
 十一归去逢重九，醉看云外月如诗。
【奖励评价】:0.7142857142857143 1.0 七言绝句(仄起仄收式) 四支韵 第24字 [外] 不符合平,第26字 [如] 不符合仄

GRPO-有CoT:
 礼乐飞花迎典庆，青枫白果待秋迟。
 十一归去逢重九，心事如诗共醉时。
【奖励评价】:1.0 1.0 七言绝句(仄起仄收式) 四支韵 
```

因为这是我自己写的诗，原始模型肯定是没有在互联网语料里见过的。

其实原始模型基础能力也很不错，但它有个大问题就是经常搞错字数，这个在CoT里可以清晰看到

```log
好的，用户让我根据前三句写绝句的最后一句。首先，我需要确认用户提供的前三句是否符合绝句的格式。绝句通常是四句，每句五个字，押韵。用户给出的前三句是：

天马斜飞度三止，上将横行击四方。辎车直入无迴翔。

看起来每句五个字，押的是“止”、“方”、“翔”，
...
```

总体来看，GRPO后的结果，平仄和韵的把握度要远高于原始模型，略高于SFT模型。

但GRPO后接的诗，相比SFT的模型，缺了一些意境相符。

比如最后一首诗，整体是一个向上的意境。
```
礼乐飞花迎典庆，青枫白果待秋迟。
十一归去逢重九，正是少年登高时。
```

GRPO前的回答尚可，至少接了重九的意象。
```
礼乐飞花迎典庆，青枫白果待秋迟。
十一归去逢重九，重阳佳节又相期。
```

而GRPO后的回答，就没有什么前后联系了。
```log
礼乐飞花迎典庆，青枫白果待秋迟。
十一归去逢重九，心事如诗共醉时。
```

**这也正是奖励函数的限制，因为只奖励了格式上的东西，模型反而丢失了原本关于意象上的东西。**


# 问题全对的问题
最后提一个实测时观察到的问题

之前分析有提过，但奖励接近时，模型的偏向会受loss计算的影响。

**但所有结果都全对时，得分完全相同，奖励权重变为0，导致无法学习。**

这会浪费一次有效学习的机会。产生了大量正确样本，却无法学习。

当然当G（每次样本生成量）特别大时，这个问题得以弥补。

但反过来，这可能也是合理的

**因为如果题目特别容易对，说明题目很简单，这样的题目也没有太多学习的必要。**

只有难题才值得去学习。

然而受限于条件，当G小时，可以考虑保存这些高分样本，用于新的SFT也是不错的。

同时另一个选择是尽量让奖励连续，或则增加奖励维度，让得分不容易完全相近。

但奖励的变化产生的影响可能会比较巨大，需要谨慎测试。

还有一个有效的工程实践，通过逐渐提升题目难度，来避免后期简单题浪费时间，我在[Xiaomi MiMo](https://github.com/XiaomiMiMo/MiMo)里看到过


# 总结
GRPO的训练，是对SFT已有成果上，按奖励函数进一步提升

**没有CoT也可以进行GRPO训练，而且因为生成长度低速度会更快一些**

然而GRPO过程中，**模型可能会过于满足奖励函数，而失去了一些原有的倾向**。

比如诗歌创作中过于满足格律要求而丢失了上下文意境的匹配。

最后，当分数整体较高时，训练会趋于缓慢。可以通过**增大生成样本，细分奖励函数，以及规划问题难度等**方法去解决。

之前文章有提过DeepSeek R1的GRPO训练，会使得CoT增长变大，但我这里训练CoT反而下降了，这里面有问题类型本身的因素，有模型的因素，也有loss type的因素，我会下一节详细分析。


---
title: "OpenR1实战(6)--Loss的样本level与Token Level"
date: 2025-06-04
categories: ["实战", "教程"]
tags: [llm, grpo, openr1]
---
上一节我们分析了不同loss的代码实现会导致不同的CoT影响。

样本level的loss，相比Token level的loss，CoT会有增长的趋势。

DeepSeekR1的论文并没有详细提及loss的工程实现。

**但从实际DeepSeekR1的超长CoT表现来看，其loss实现具有上升CoT的倾向。**

**而OpenR1，以及其依赖trl库中，loss的实现与上述倾向不一致。**

本节，我们尝试在trl原有基础代码上，实现样本level的loss，并与trl原始的loss方式进行实验对比。

## 概率的计算
问题核心在于如何计算

$$
\mathcal{J}_{\text{GRPO}}(\theta) = \mathbb{E}[q \sim P(Q),\, \{o_i\}_{i=1}^{G} \sim \pi_{\theta_{\text{old}}}(\mathcal{O}|q)] \\ 
\frac{1}{G} \sum_{i=1}^{G} \left( 
\min\left( 
\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)} A_i,\ 
\operatorname{clip}\left( 
\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)},\ 1 - \varepsilon,\ 1 + \varepsilon 
\right) A_i 

\right) 
- \beta \mathbb{D}_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}}) 
\right) 
$$

先忽略掉期望和Clip那段，整个loss计算由相对概率和散度计算构成。

$$
\frac{1}{G} \sum_{i=1}^{G} \left(\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)} A_i 
- \beta \mathbb{D}_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}}) 
\right) 
$$

散度的实现也有很多，而且与CoT无关，我们就沿用trl的实现方式。

$$
\frac{1}{G} \sum_{i=1}^{G} \left(
    \frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)} A_i 
\right) 
$$

因为数值精度问题，工程代码里会以log概率作为中间存储。

$$
\frac{1}{G} \sum_{i=1}^{G} \left(
    \frac{exp(log(\pi_{\theta}(o_i|q)))}{exp(log(\pi_{\theta_{\text{old}}}(o_i|q)))} A_i 
\right) 
$$

exp变成减法

$$
\frac{1}{G} \sum_{i=1}^{G} \left(
    exp(log(\pi_{\theta}(o_i|q))-log(\pi_{\theta_{\text{old}}}(o_i|q))) A_i 
\right) 
$$

log概率已经计算出来了，所以可以简化为
$$
\frac{1}{G} \sum_{i=1}^{G} \left(
    exp(logP(o_i)-logP_{old}(o_i)) A_i 
\right) 
$$

也就是 `per_token_logps`

$$
logP_{pertoken} = [logP(t1|q), logP(t2|q,t1), ....]
$$

这是直接计算的每个token的对数概率。

核心问题来了。

**每个token的概率如何变成整个样本概率**

数学上考虑的话，联合概率就是各个独立概率的乘积

所以公式可以变为

$$
\frac{1}{G} \sum_{i=1}^{G} \left(
    exp\left( \frac{1}{|o_i|}\sum_{t=0}^{|o_i|}(logP_{pertoken}(t_i))-\frac{1}{|o_i|}\sum_{t=0}^{|o_i|}(logP_{old-pertoken}(t_i))\right) A_i 
\right) 
$$

**这种做法就是样本level的，以整个样本的联合概率作为loss计算的标准。**

转换成Code的话为

```python
my_coef_1 = torch.exp((per_token_logps * completion_mask).sum(-1) - (old_per_token_logps * completion_mask).sum(-1))  # B,1
```

而trl的loss则是token level的

```python
coef_1 = torch.exp(per_token_logps - old_per_token_logps) # B, token_num
```

最后再对整个相对概率求和乘以奖励

转换成数学公式是这样的

$$
\frac{1}{G} \sum_{i=1}^{G} \left(
    \frac{1}{|o_i|}\sum_{t=0}^{|o_i|}exp\left(logP_{pertoken}(t_i)-logP_{old-pertoken}(t_i)\right) A_i 
\right) 
$$

**这就变成了一种奇怪的数学意义——用概率均值来表征样本概率。**

这种概率, 某种程度上是有道理的，因为

**1 这里用的是相对概率而非绝对概率，大部分值都靠近1**

**2 在样本长度和token概率接近时，概率均值可以接近真实概率**

总的来说，求和概率在一定程度上能够表征整个样本的相对概率。

但上节我们也分析过，**如果样本长度差距很大时，求和概率会造成不同的影响。**

这么看似乎是用乘积模式的样本loss会更贴切一些？

**但使用和模式的token level loss有另外的工程优势。**

## Token Level的优势

### 工程梯度，加法比乘法好
虽然在logP的乘法可以表现为加减法。

但回到概率计算时，仍然是乘法。

乘法的梯度需要其他输入变量的乘积结果

这个计算不复杂，但是会导致梯度变得不受控制。

而Token Level则没有这个问题，每一个Token的梯度都是自己单独计算的。

我们有一些办法让梯度更受控制，**但整体而言，都没有加法那么易训练。**

### Token的可理解性
与其他类型的模型不同，语言模型的输出Token是字词，是具备可理解性的。

人可能记不住数字“10534”，但能够认识字“你好”。

也就是在训练过程中，**控制Token的概率是有价值的，以便让模型生成更易理解的语言文字**。

而样本level会抹平这一切，中间输出的Token不在关心，只关心最终结果。

比喻来说，模型可能倾向于生成一种自己独特的语言或者表达习惯，让人类不是那么好理解。过去有些研究提到这种AI自创语言的故事，可能就是在强化学习时loss没有配置好。

## 样本 level的劣势
Token level的优势也就是样本level的劣势

**所有的Token在LogP的梯度在样本里是抹平的。**

一些频繁出现的字词会有更高的出现概率。

**原本这些字词的概率要受old概率分母限制，而在样本level的计算里，限制是所有token一起承担的。**

高频Token容易变得更加高频。

我不知道DeepSeek是如何做到既有样本层面的概率控制，又避免单个Token的概率被拔高，也许KL散度的控制也很关键。

# 实验
根据上文公式，实现样本loss计算代码如下

```python
elif self.loss_type == "my_grpo":
    my_coef_1 = torch.exp((per_token_logps * completion_mask).sum(-1)
                            - (old_per_token_logps * completion_mask).sum(-1))  # B,1
    my_coef_2 = torch.clamp(my_coef_1, 1 - self.epsilon_low, 1 + self.epsilon_high)  # B,1
    my_coef_3 = torch.min(my_coef_1, my_coef_2)
    if self.beta != 0.0:
        loss = -(my_coef_3 * advantages - self.beta * (per_token_kl * completion_mask).sum(-1)).mean()
    else:
        loss = -(my_coef_3 * advantages).mean()
```

## 曲线
CoT长度和bnpo一样会有明显上升

![](/assets/images/2025-06-04-GRPO实战6-Loss的样本level与Token_Level-CoT长度的对比.png)

而reward增长要相对缓慢一些

![](/assets/images/2025-06-04-GRPO实战6-Loss的样本level与Token_Level-reward增长对比.png)

如果时间更长，reward依旧能达到很高的值（因为很耗时间，其他的losstype我没有都做测试更长时间的训练）。

![](/assets/images/2025-06-04-GRPO实战6-Loss的样本level与Token_Level-更长时间的reward.png)

而CoT也是会按最大Token数（4096）进行波动。

![](/assets/images/2025-06-04-GRPO实战6-Loss的样本level与Token_Level-更长时间的CoT长度.png)

## CoT内容
在4000多global_step后

my_grpo的CoT变得很异常了。虽然给的诗句都很正确，但是CoT里在反复的重复相同Token。

```log
<|im_start|>user
闲韵虽高不衒才，偶抛猨鸟乍归来。夕阳照箇新红叶，<|im_end|>
<|im_start|>assistant
<think>
好的，用户给了绝句前三句“闲韵虽高不衒才，偶抛槃鸟乍归来”，需要我根据平仄和对仗工整来补全第三句和第四句，只回复最后一句。对仗工整工整对仗工整工整工对，符合平仄与对仗工整工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对。

用户给出的绝句结构是三句对仗工整工对工对工对工对工对工对工对工对工对工对工对工对工对，对仗工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对，工对对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对，工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对。第三句工对工对工对工对工对工对工对工对工对工对工对工对工对，对仗工对工对工对工对工对工对工对工对工对工对工对工对工对工对对工对工对工对工对工对工对工对工对工对工对工对工对工对对对工对工对对工对工对工对工对工对对工对工对对工对工对对工对对对工对对对。

首联工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对，对仗工对工对工对工对工对工对工对工对工对工对工对工对工对工对工对对工对工对对工对工对对工对工对对工对工对对工对对对工对对对工对对对工对对对工对对对，工对对工对工对对工对工对对工对工对对工对工对对工对对工对对工对对对。对仗工对工对对工对工对工对对工对工对工对工对工对对工对工对对工对对工对对工对对工对对。

对仗工对工对工对工对工对工对对工对工对对工对工对对工对对工对对工对对工对对工对对工对对工对对工对工对对对工对对工对对工对对对工对对对工对对对工对对对，工对对工对工对对工对对工对对对工对对对工对对对工对对对工对对对工对对对。

对仗工对工对对工对工对对工对对对工对工对对工对对对工对对对工对对对工对对对工对对对工对对对工对对对工对对对工对对对工对对对，工对对工对工对对工对对工对对对工对对对工对对对工对对对工对对对工对对对。

所以第三句工对对工对工对工对工对对工对工对对工对工对对工对对，工对对工对工对对工对工对对工对对对工对对对工对对对工对对，工对对工对工对对工对对对工对对对工对对对。对仗工对对工对对工对对工对对工对对工对对工对对工对对工对对工对对工对对，对仗工对对工对对工对对工对对工对对工对对工对对工对对工对对工对对工对对工对对工对对工对对工对对。

对仗工对对工对对工对对工对对工对对工对对工对对工对对工对对工对对工对对对工对对对工对对对工对对对工对对对工对对对工对对对对。

这样对仗工对工对对工对工对对工对对对工对工对对工对对对工对对对工对对对工对对对工对对对对工对对对对工对对对对工对对对对工对对对对工对对对对工对对对对工对对对对工对对对对工对对对对工对对对对对对。

用户要求符合平仄与对仗工对对工对对工对对工对对工对对对工对对对工对对对对工对对对对工对对对对对工对对对对工对对对对工对对对对对工对对对对工对对对对工对对对对对工对对对对工对对对对对工对对对对工对对对对工对对对对。

接下来对仗工对对工对对工对对工对对工对对对工对对对工对对对工对对对对工对对对对工对对对对工对对对对对工对对对对工对对对对工对对对对对工对对对对对工对对对对工对对对对对工对对对对工对对对对工对对对对对工对对对对工对对对对对工对对对对工对对对对工对对对对对。
</think>

闲照光同对雨裁
```
其它的loss里没有这种现象，但我只训练到了1.5k的global_step

bnpo的CoT偏长

```log
<think>
嗯，用户让我写一个绝句，前三句和最后一句。得按照平仄和押韵。用户给出的前两句是“金吾持戟护新簷，天乐声传万姓瞻。”，我需要接第三句和第四句，但用户说只回复最后一句，所以得先看用户给的最后两句。

用户给出的第二句是“天乐声传万姓瞻”，第三句是“楼上美人相倚看”，最后要接。现在要填第四句。平仄和韵要符合。

用户提供的第二句是“金吾持戟护新簷”，第三句“天乐声传万姓瞻”，第四句“楼上美人相倚看”。这里需要注意押韵和平仄。用户可能期望押平声韵，比如“瞻”和“看”或者“瞻”和“声”。

首先分析原句的结构。第一句和第二句都是对仗的。平仄方面，第一句“金吾持戟”是仄起句，第二句“天乐声传”是仄起句，第三句“楼上美人”是平起句。不过需要更准确的平仄。

“金吾持戟护新簷”平仄是仄平仄仄平，对仗。第二句“天乐声传万姓瞻”平仄是平仄平仄仄，对仗。第三句“楼上美人相倚看”平起句，平仄为仄仄平平仄。

然后第四句需要接，押韵。比如“清寒梦入画中宵”这样。但要注意和第三句的“相倚看”形成对仗。

再检查平仄是否一致。比如第一句：“金吾持戟护新簷”是仄平平仄仄，第二句“天乐声传万姓瞻”平仄仄平仄。第三句“楼上美人相倚看”仄仄平平仄。第四句“清寒梦入画中宵”平仄仄仄仄。这样结构对仗。

押韵的话，前两句“瞻”和“瞻”是同一韵，第三句“看”和“宵”是另一个韵，但可能需要更准确的韵脚。或者“瞻”和“声”“宵”“看”押同韵。

比如第四句“清寒梦入画中宵”中的“宵”是“瞻”的韵，或者“宵”和“瞻”都是平声，押“瞻”、“声”、“宵”。

所以最后的韵是“瞻”、“声”、“宵”。或者“瞻”、“声”、“看”。

然后检查平仄是否符合。第一句：平仄仄平仄，第二句：仄平仄仄平，第三句：平仄平平仄，第四句：仄仄仄平仄。

这样结构对仗，押韵一致，平仄正确。

比如第四句：“清寒梦入画中宵”对仗，押“宵”是平声，和第三句“相倚看”的“看”押韵。

所以综合起来，最后一句应该是“清寒梦入画中宵”，符合平仄和押韵。
</think>

清寒梦入画中宵
```

grpo的CoT偏短

```log
<think>
好的，用户让我生成一首绝句，前三句是“不悲霜露但伤春，孝理何因感兆民。”接下来要填最后一句，要符合平仄和韵。首先，分析前两句的结构和意象。前两句讲的是不悲伤霜露，这里“霜”和“春”可能用作韵，比如“霜”作为首句的韵，后面要找一个类似的韵脚。平仄方面，前两句是“不悲霜露”是平仄平平仄，后一句“孝理何因”是平平仄仄仄。但需要检查这些字的声调是否符合规则，比如“不悲”是平平仄仄平，可能需要调整。然后考虑后句的意境，比如“愿共天人长共吟”这样，押平水韵，平仄对称，同时表达对国家关怀的主旨。最后检查是否符合古诗的格律，确认韵脚和平仄正确，确保整首诗的意境和情感传达得当。
</think>

愿共天心共岁吟。
```

因为GRPO训练实在耗时，无法做更多测试去比较了。不同loss都是在相同的超参数测试的
```yaml
gradient_accumulation_steps: 2
num_iterations: 2
num_generations: 4
beta: 0.00
```

## 梯度
前文已经分析过，因为样本loss会抹平梯度，导致高频token变得更多。

我们可以打印backward过程中的梯度，很清晰的观察到这一点

per_token_logp的梯度比较

![](/assets/images/2025-06-04-GRPO实战6-Loss的样本level与Token_Level-梯度比较.png)

# 总结
在上一节分析完loss对CoT长度的影响后

本节进一步分析了样本loss与Tokenloss对训练的影响。

在实验中，发现样本loss的训练，依旧可以达到很高的正确率，但是CoT会变得重复且难以理解。

通过分析梯度，我们得知以样本loss计算的梯度，会抹平所有token在logp层的梯度，从而导致部分高频token以更高的频率出现。

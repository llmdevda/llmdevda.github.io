---
title: "梯度训练，信息熵与巫术占卜"
date: 2025-06-05
categories: ["实战", "教程"]
tags: [llm, grpo, openr1]
---

这篇文章最早想用一个极简模型解释一下GRPO的梯度下降为什么导致熵增。

**以及熵增为什么会有价值。**

当时想到了占卜，仔细设想了下感觉这个例子很有意思。

以这种极简形式去做数学推导，引入简单的0-1概率，也能得到很符合现实的结论。有机器学习背景知识的朋友可以直接看公式的那一部分。

## 萨满与木块：原始版机器学习

> 很久很久以前，在一片湿热多变的丛林里，住着一个以打猎为生的原始部落。风雨莫测的天气像命运的筛子，每次狩猎都赌上了收成甚至性命。
> 
> 幸运的是，部落有一位萨满，他能观天象、测阴晴。每天清晨，他削好木块、焚香祷告，为族人占卜今日是否宜出猎。晴天，他们就出发；雨天，则窝在部落避祸。
> 
> 但萨满并不是神。为了能让预测更准、获得更多食物奖励，他开始调整自己的占卜方式。他逐步从经验走向“学习”，从直觉走向“算法”……就这样，一个原始世界中的“梯度训练”故事悄然展开了。

想象一个远古部落在丛林中，部落以打猎为生。

这里气候非常突变，如果在打猎过程中遇到突降的暴雨，轻则颗粒无收，重则损失人员。

部落里有个萨满负责占卜天气，如果天晴则部落集体外出打猎，如果下雨则留在部落不外出。

萨满每天早上作出占卜，q的概率为天晴，1-q的概率为下雨。

部落人根据萨满预测是否准确来给萨满奖励食物ri。一段时间下来，萨满就得到总奖励ri求和

## 一天一削：在线学习的苦与甜

萨满为了得到更多食物，就得尽可能占卜正确

随后根据是否下雨，调整概率q的值

如果今天是天晴，那么把q调大一点。

比如如果今天下雨了，那么把1-q调大一点，也就是q调小一点。

这就是梯度训练。

萨满用这方法改善预测，虽然不是很准，但也大致符合了现实。

> 这种方式，其实在技术上叫做 在线学习（online learning），也就是模型每接受一个新样本就更新一次参数，相比那些要等大批数据才训练一次的方式，它更灵活，也更能快速适应环境变化。

## 批量削木：Batch 的诞生

但萨满发现每天都去调整概率太麻烦了——每天占卜就要每天削一次木块，木块紧张，时间也紧张，手还容易削出血。

于是他决定改个方式——一周收集几天的结果，再集中削一次。

比如这周一连出了三天太阳，他就在周四一次性把“晴天”的概率调大一点。

这样既节省木块，也少削点手，效率还不错。

这就是训练中的 Batch 更新，或者梯度累积（Gradient Accumulation）

> 一天一削太敏感，今天晴天就大幅度调高晴天概率，明天下雨又反手调回，模型学不到长期趋势。
批量更新相当于把最近几天的情况平均考虑，减少了波动，更“理性”。
> 
> 对萨满来说是节省木块，对我们来说是减少频繁计算，特别在大模型训练中能减轻显存压力。比如一次喂不下大 batch，就先分小段“记账”，最后统一“结算”。

## 天地异象：多解的困惑

有一天，天生异象

这天不仅下雨而且还出太阳，在部落下雨，在丛林就出太阳。

**于是，萨满无论怎么预测都是正确的。**

部落送来了满满的食物，他第一次感受到“预言全中”的快感。

那么这时？萨满怎么去调整概率最好呢？

## 极端自信的陨落

我们来讨论一个方案。

**首先是一个固定梯度的方案的导向极端的方案，**

因为无论预测哪种都是正确的，那么萨满做出任何预测都会依据提升改预测概率方向的梯度进行。

如果此时某一方的概率大，则其出现在占卜预言的次数多，其提升的也就越多。

长此以往，概率就会走向极端。

**比如萨满每次预测晴天都发现对了，雨天也发现对了，他就愈发的提升预测晴天的概率，直到一直预测晴天。**

一切都很顺利，直到有一天，异变消失，新的一周全是雨天。

萨满的预测全部失败，预测失败就没有食物，萨满因为一周都没有食物而饿死了。

## 怀疑一切的萨满

见证完极端方案后，我们看看**相对概率的方案**

相对概率时，梯度会按当前概率的倒数。

现在预测晴天的概率q，如果今天晴天，那按1/q的幅度比率去提升q的值。

比如现在q为0.1，也就是10%的概率预测天晴，那么提升的幅度比率是10

而如果现在q为0.5，那就是50%的概率，那么提升的幅度比率为2

**概率越高时，调大的幅度越小。**

**概率越低时，调大的幅度越大。**

这种策略会导向更加中庸且怀疑的选择。

随着异常天数的增加，概率会趋于一半一半，q=0.5。

萨满不确信是晴天还是雨天，他对两种天气保持相等的怀疑。

然后又到了异变消失的一天，新的一周每天都是下雨

**上一个萨满因为全预测晴天而饿死了，而新的这个中庸且怀疑的萨满，有概率能活下来，因为他会去预测雨天的可能性。**

这就是不同梯度策略导致的不同结果

或者说不同的应对变化的结果。

当然，如果我们把条件设置的严苛一点，比如每周必须每天都吃饭才能活下来，那么可能任何策略都难以起效。

**所以策略的结果也是取决于环境的严苛程度的。**

在某些环境下，这种中庸的策略在过程中可能就死了，就像我们知道有时侯就得不犹豫的一股脑冲，尤其是在一些以小搏大的场景。

**所以我们很难判断策略好坏。**

**只能说限定在易变换且具备一定容错的环境下，偏中庸相对概率梯度策略，更不易走向极端而连续失败。**

## 信息熵与存活几率

中庸的概率策略其实在数学上会表现为信息熵的增大。

要计算概率分布的信息熵，我们可以使用信息熵的公式：

$$
H(X) = -\sum_{i} p_i \log_2 p_i
$$

其中，$p_i$ 是第i个事件的概率，在这里就是预测晴天与下雨。

1 对于纯预测晴天的萨满 概率分布 \( (0, 1) \)

$$
H(X) = - (0 \log_2 0 + 1 \log_2 1) = - (0 + 0) = 0
$$

他的信息熵是 **0 比特**。

2 对于平均预测的萨满 概率分布 \( (0.5, 0.5) \) ：

$$
H(X) = - (0.5 \log_2 0.5 + 0.5 \log_2 0.5) = - (-0.5 - 0.5) = 1
$$

他的信息熵是 **1 比特**。

也就是说，中庸的萨满2号，比自信的萨满1号具有更高的信息量。

**更高信息量的策略，在应对不确定环境时，有更高的正确机会。**

这里**并不是正确概率，因为正确的期望是一样的。**

但是萨满的时间不是无限的，在有限的时间里，中庸的策略更有机会正确。

**这种机会并不是指概率期望，因为从期望而言，猜测正确与否的成功率还取决于老天。**

我们用数学公式表达一下

$$
q - 表示预测天晴的概率 \\
q_r - 表示实际天晴的概率
$$

那么预测正确的概率就是1-预测错误的概率

$$
p_{correct} = 1-(q(1-q_r) + (1-q)q_e) \\
= 1- (q + q_r - 2qq_r )
$$

N次中至少正确一次的概率为1-全错的概率

$$
p_{correct} = 1-(q(1-q_r) + (1-q)q_e)^N \\
= 1- (q + q_r - 2qq_r )^N
$$

当一直猜晴天时，N次至少正确一次的概率为 

$$
1-(q_r)^N
$$

当一直猜雨天时，N次至少正确一次的概率为 

$$
1-(1-q_r)^N
$$

当随机猜测晴天雨天时，正确的概率则与老天无关，而是0.5，N次至少正确一次的概率为 

$$
1-(0.5)^N
$$

当$q_r$从0突变到接近1时，一直猜某面的N次至少正确概率会降低。

**而当突变为1或者0时，单向猜测的正确概率会趋近于0。**

**而随机猜测的N次正确概率则在突变中保持了一致。**

这就是2号萨满的效果，在环境突变时，其预测正确率取决于自身而非外部环境。


## 策略的赌局：运气、环境与生存
> 在风云诡谲的大自然面前，极端的自信有时能赢得丰收，但也可能付出饿死的代价；而中庸的怀疑虽不激进，却更容易在长期的不确定中幸存。
> 
> 信息熵高的策略并不会提高“期望准确率”，但却能在有限试错中避免一次次连败的悲剧。在真实的世界里，我们也常常不是输给了错误，而是输给了连续的错误。
> 
> 所以，无论是萨满占卜、模型训练，还是现实决策，中庸并非妥协，而是一种对不确定性更理性的响应。在环境变化剧烈、容错有限的时代，也许那个不全信晴、不全信雨、始终保持怀疑态度的萨满，才最有可能，活到最后。

我们总以为正确的选择是找到答案，其实很多时候，更重要的是判断什么时候我们不能确信答案。

在模型训练中，策略往往并不败于误差本身，而是败于陷于局部最优。

尤其是在反馈混沌或极端稀疏的场景中，哪怕一次判断有利，也可能是“误判中的胜利”，从而导致模型被带往不可逆的极端。

现实中的挑战往往就是混沌与稀疏的。

这就是为什么信息熵在这里变得有意义——它不告诉你“什么是对的”，但提醒你“你是否过于自信”。**高熵策略不会提高你的期望得分，但会降低你被极端情况击垮的风险。**

“中庸”不是保守与逃避，而是一种主动的自我保护。

**它不属于追求精确，而是追求稳定的发散。**

**它不追求完美模型，而是利用概率，寻求更稳定的长期。**

现实世界充满了黑天鹅。

当我们企图将模型应用的现实问题的解决中，我们必须找到“正确”与“怀疑”的中间点。

也许很多“聪明”的模型，并不是更聪明，而只是更愿意承认：**我不知道**。

这也许不是通向AGI（通用人工智能）的道路，但在风变云诡的现实中，它却是通向下一轮迭代的前提。





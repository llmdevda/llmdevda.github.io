---
title: "当AI学会"反复打脸自己"，人类该怎么做？"
date: 2025-05-19
categories: ["原理", "科普"]
tags: [llm, `GRPO`, deepseek]
---

当强化模型学会反思，人类的未来会怎么样？

2022年年末，`OpenAI`的`ChatGPT`进入到人们视野中，其近似真人的对话能力让人震惊。

然而`OpenAI`并未开放它的模型和训练方法细节。`OpenAI`因此也被戏称CloseAI。

2025年年初，`DeepSeek`的R1模型横空出世，其生成速度和模型评分超过当时如日中天的`OpenAI`的闭源o1模型。

**这个结果非常振奋人心！**

那么`DeepSeek`，是如何做到这一切的呢？

`DeepSeek`在它的DeepSeek_R1等论文中提到了相关的细节。

包括更长的上下文，更高效的int8计算，以及适合多结点的强化训练方式`GRPO`。

其中，工程能力和更长上下文成为了基础，让模型具备足够的能力基础。

**而`GRPO`则是作为核心的——让模型更具智慧的一步。**

`GRPO`的全称是Group Relative Policy Optimization，群组相对优化学习。就是这个学习方法，让`DeepSeek`的R1模型，超越了当时的`OpenAI`。

## 如何选更好的学生？
首先，思考一个问题

**当两个学生都考了90分，怎么判断谁更好？**

这个问题是模型进一步学习的核心。

**进一步学习前，模型往往进行传统的训练，叫基座模型训练。**

**这个过程就是模型去模仿人类写文字。**

将互联网和历史书籍里的所有文字资料，喂给模型，让它模仿写文字。模型时以文字概率的方式表达，比如上一个字是“你”，下一个字有可能就是“好”，有可能是“们”，模型就把所有字可能出现的概率计算出来，然后选择概率比较大的那些字。

**于是有人质疑，一个纯靠模仿人类的AI，真的能超越人类吗？**

实际表现也是并不很聪明，ChatGPT刚出世那会，很多人第一感觉都是它很擅长一些已有知识的问题，但对于新问题和没出现过的问题表现很差。

常理来说，仅靠模仿也很难做到更好。

**但若让模型自己去思考呢？**

强化学习就是这样

我们熟知的AlpheGO就是，通过不断与自己下棋，超越了人类。

**怎么让大模型和强化学习搭边的呢？**

**做题。**

**做数学题。**

数学题对了就是对，错了就是错。

通过这个办法，模型的数学解题能力蹭蹭上涨。

但是很快到了一个瓶颈。

有的数学题简单，有的数学题真的很难

**结果就容易题都作对了，难题全不会。**

这时怎么办？

这也是最开始提到的核心问题，都是90分，怎么办？

各位也可以停在这里思考一下自己的答案。

一种思路是，**让模型写出自己过程，叫思维链CoT(Chain of Thought)**，然后提供一个阅卷老师，用另一个高级模型做老师。

**虽然学生答错了，但老师能根据学生的思考结果判断谁更有潜力。**

![](/assets/images/2025-05-19-`GRPO`的大白话解释-图中的部分就是思维链.png)

但如果老师也不够聪明怎么办？

**如果老师根本不存在怎么办？**

**DeepSeek探索出来一条路，叫GRPO。**

我们这里不说细节。

直接说结论——**追求思辨**

## 思辨

**怎么判断哪个学生更思辨呢？**

我们提到模型是概率模型，**我们可以去计算模型对自己答案的置信度**，也就是它对自己答案有多自信。

当两个学生都一样分

**我们偏向选那个更不自信的学生。**
更**怀疑自己**答案的学生。

举例来说

> 提问 1 + 1 = ？
> 
> 小王：这么简单，显然是3<br>
> 小红：这么简单，显然是2<br>
> 小明：虽然三个符号，所以是可能是3。但我发现这里有两个1，中间是+号需要累加，所以答案是2<br>
> <br>
> 小红和小明都对了，都是你们的榜样，但是小明思考的更长更纠结一点，所以你们要更向小明学习

就像上面例子一样，模型会选择小明的这类思维。

**这类学生思维有个特点，它们思考的特别长，而且总怀疑自己之前的结论**。

我们鼓励向它学习，而且还鼓励越来越怀疑自己。

它虽然小题做的慢一点，但是考虑的更多。

这让碰到难题时，更有可能做出来

`DeepSeek`在训练模型时，增加了这一隐藏的价值观。

让模型在做题越来越厉害的同时，保持了一种自我怀疑的倾向。

**追求思辨**

其表现就是`DeepSeek` r1论文提到的顿悟时刻——重新思考

**模型不停的否决自己，不停的重新思考**

从而变得更加思辨

`DeepSeek`模型出来后，一下就在数学题上战胜大部分其他模型。
靠的就是这种思辨能力

![](/assets/images/2025-05-19-`GRPO`的大白话解释-`DeepSeek`对比`ChatGPT`的思维链例子.png)
(上图是`DeepSeek`与`ChatGPT`的思维链对比，可以看到`DeepSeek`考虑的更多，而且不停推翻前面思考，考虑更全面)

## 为何要思辨？

从数学角度，提升思辨也就是提升了概率的不确定性。思辨的模型会更多去探索不同的选择，从而更容易去解决未出现的问题。

**自我怀疑模型会更容易举一反三。**

反之，如果我们不是选择思辨的策略（概率低），而是选择自信的策略（概率高）。在新一轮的运行中，自信的策略会被更容易被选择，从而收敛到一个局部空间上。

**而过于自信的模型容易盲目自信。**

就像
有的小孩完全相信课本结论，

有的小孩会去质疑老师和课本的结论，

有的小孩甚至会质疑自己的结论。

## 局限性
**但事物总有双面性，**

思辨也可以换个差点的词——**瞎想**

**所以我们会见到有一些简单问题，模型不停瞎想，然后给出一个错误答案。**

![](/assets/images/2025-05-19-`GRPO`的大白话解释-猜灯谜的例子.png)
（就像上图例子里，模型最早就想到了正确答案，但持续怀疑自己，最终给出了错误答案）

就像现实里，能背题的小孩往往也挺聪明。聪明小孩不想太多，但想的很巧妙。就像数学几何题，画了那条辅助线马上就能做，没画辅助线想破脑子也做不出。

**我们必须再次提问，增加思辨能力真的可以提高模型智慧吗？**

让我们稍稍跳出模型，

评价一小孩，

一小孩每次考试都打很多草稿然后写答案，另一小孩凭直觉就能迅速写出答案。

你觉得哪个小孩更聪明？

进一步，

评价一个人，

一种人做事想很多再决定，另一种人凭直觉迅速决定。

你觉得哪种人做事更靠谱？

**我的想法是无法判断，**

我认为不能神话`GRPO`，

对于知识与认识我们了解的甚少，增加一个人的思考时间并不见得让他变得更智慧，COT并不见得是未来的道路

而且我们有理由怀疑，模型在学习过程中不是增加了智慧，而是增加了对思辨文风的模仿。

但无论如何，`GRPO`探索出一条道路，其潜力可以让模型智慧更上一层楼。

比喻来说，没有天才儿童，也没有优秀老师时，让小孩学着思辨，也许是学习的好法子。

## 思辨与人
总而言之，思辨是`DeepSeek` r1的目标

思维链增加的文本量，本质也是增大思辨的量

一个有意思的事，r1这个词，输入法打出来是——人

`DeepSeek` r1似乎抓住了人类智慧的一个关键——思辨

**三省吾身，思辨带来的自我怀疑让模型表现出更强的智慧能力，而且更易训练，这指名了一个方向。**

思辨是进步的阶梯，不断的怀疑与内省让人进步。

就像这篇文章一样，我也在不停怀疑自己的结论。而且很有可能在不久之后我就认为自己是错的

为什么呢

**因为人类还有一个更重要的能力——实践**

实践到理论，再到实践，这是认识的来源、动力与基础。

**思辨是获取真理的引路石，而实践是检验真理的唯一标准。**

AI模型的出现，是历史上所未有的，崭新的生产工具。

面对这个全新的工具，我们去实践，去理解，去检验，就是最好的认识方法。

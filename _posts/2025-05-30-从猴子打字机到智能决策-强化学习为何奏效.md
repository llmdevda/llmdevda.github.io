---
title: "从猴子打字机到智能决策：强化学习为何奏效？"
date: 2025-05-30
categories: ["大白话", "llm", "rl"]
tags: [llm, grpo, openr1]
---

DeepSeekR1出来之后，我一直在思考一个问题

**RL（强化学习）凭什么能带来智慧？**

> 强化学习：让一个智能体在与环境交互的过程中学会做出最优决策，以最大化未来的累积奖励。

或者说，SFT（监督微调）模型凭什么能在RL（强化学习）后突破智能上限？

SFT是对样本的模仿，而RL则是通过奖励，把新生成样本分为正负样本

具体而言，RL后的模型凭什么能做对一道它从没见过也跟样本不具备相似度的数学题？

一种解释是**猴子打字机，你让猴子去瞎写，它总有一天能做出这个题答案**。

> 无限猴子定理（英语：Infinite monkey theorem）的表述如下：让一只猴子在打字机上随机地按键，当按键时间达到无穷时，几乎必然能够打出任何给定的文字，比如莎士比亚的全套著作

我不相信这个，我们**有限的训练时间显然无法做到无限的回答**。

但猴子是个很有意思的概念，我们借着猴子，来探讨一下RL与智能。


# 养蛊
**离散奖励的群组RL就像在养蛊。**

LLM的RL就是在养猴子。

将一堆猴子放一块打字，挑一个最厉害的猴。

但我们知道，养蛊本就不科学，某一个虫子可能天生就克制另一个虫子。

猴子也一样，也许有的猴子它不是不会做题，只是不会读题。

**最终可能出来一个很笨的结果。**

比如猴子A在永远打答案是2，

而猴子B在写文声讨自己失去自由只能在这暗无天日的房间里敲打字机，

而猴子C写了一篇文章叫《猴子打字机与强化学习RL凭什么有用》这种意义不明的文章。

很显然，在数学题方面，猴子A的正确率会远高于猴子B和猴子C。

于是猴子A成了最聪明的猴。

**为了避免这种情况，我们对猴子B C进行了洗脑（注意力微调）。**

让猴子B C不要瞎写作，老老实实的写数学题。

**另外只要猴子B C不好好答题，我们就电疗它们（RL中的格式惩罚）。**

吓的猴子们再也不敢乱答题，智能老老实实写数字。

这种情况下，猴子B和C的正确率会比猴子A要高。

因为猴子BC本来就比猴子A聪明。

那么问题来了，

**猴子B C究竟是变聪明了？还是变得更听话了？**

所以，如果不去仔细考虑RL的机制细节，结果会混淆我们

无法分清，**RL只是让模型有所偏向，还是RL确实提升了领域能力。**

或者，这二者本就是一样？

提升了模型某能力的偏向，不就是提升了它的实际能力表现吗，所以偏向就是能力吗？

**让一个模型偏向生成数学题的解，就是提升了它的数学能力吗？**

上面这个问题像一个哲学问题，从实证角度、功利角度、认识角度可能会给出不同的回答。

但我们先绕过这个偏向问题。

走向另一个极端，没有偏向。

我向猴子提出要求——“不要停下打字机，而且你必须要在任何一个问题上，做的比现在更好”

**这时，我们要求一个模型无偏向，且提升能力。**

**一个模型无偏向的提升了能力，是否就是AGI（通用人工智能）？**


# 围棋与AGI
对研究者而言，RL有着难以拒绝的吸引力。

这种吸引力可以归根于一句话

**“你只管给Reward，剩下交给RL就行了！”**

我们就像当了老板一样，只要给下面提要求，等着做完就行了。

这是一种不切实际的幻想，但很有吸引力。我也是被吸引的一员，否则我也不会在这写这文章了

RL的这种魅力，是其现实成绩所推动的。

在游戏模拟，物理控制等一些非线性优化问题中，RL都有着不错的表现。

当然也有AlphaGo这种明星AI，它甚至改变了围棋生态。

所以，至少在围棋领域，RL的模型，超越了学习的棋谱，超越了人类，创造出全新的结论。

据此种种，我们有理由相信，RL，能行！

但，真的能行吗？

围棋的RL训练和大模型的RL训练一样吗？

围棋AI的输入可能是当前的棋局状态，而大模型的输入是文本，也可以棋局以文本形式输出。

而围棋AI输出是下一个棋子的位置概率，而大模型输出为文字概率，某种意义上，也可以文本概率表达落子概率。

我们还是拿猴子来试试。

我们相信有两种模式的训练对围棋智慧产生了影响

**第一是背谱，将已有的棋谱数据喂给AI，让其能做到基础的下棋而不是乱下**

**第二是对弈，两个AI互相下棋，学习胜者的下法。**

现在轮到猴子。

我们让猴子A和猴子B看棋谱，然后学习打字后面一棋怎么下

然后让两猴子对下，选赢的那个法子继续学习

那么问题来了，两背了谱的猴，搁那一直下棋，真能下赢人类吗？

学围棋的AI，在棋谱阶段，可能就学到了一些规律。然后在对弈阶段，发现了更多下法。

**背谱的猴，学会了背谱。到底是学会了还是背会了？两猴之后对着下，会发现更多下法吗？**

学会与背会是一个过拟合问题，现在很容易解决，我们有评估集，也有控制训练量，还有模型偏离度等方法来避免过拟合。

**所以真正的问题在与猴子是否会发现更多下法。**

**“发现更多”**

**RL的关键就隐藏在这里。**

两猴能不能下出一些新花样？

怎么下出新花样？

两猴如果一直照着背谱来下，那显然无法改变。

就比如碰到那种必死的局，猴B看棋谱没有任何方法，只能默默等输。

直到一只猴子开始瞎打，偶然地打出神之一手。


# 回归瞎打的猴子
下棋是瞎下。

打字是瞎打。

**但只有这个“瞎”，才能让模型，在原有的能力上更进一步。**

**某种意义上，也是打破旧认知。**

就是“瞎”这个带一些随机性的东西，在RL中起了关键作用。

在LLM中，在CoT（思维链）中，那就是“瞎想”

在训练猴子们做题时，鼓励猴子去瞎想。

**这种瞎想增大了信息熵，从而让猴子更有机会创造新的文字认识。**

但是，瞎打回到了打字机问题

时间并不是无限的，我们只能在有限的时间训练这猴。

这猴也只能在有限的时间去发散瞎想。

**所以我们要做有效率的瞎想。**

**这种瞎想需要发散，但不能太发散。**

而且最好还是通用的，而不是人工指定发散规则，这样它并没有一个明显的偏向（思路一xxx，思路二xxx这种模式）。

**真的有这种思维方式吗？**

还真有

**【自我否定】**

每次思考都在否定自己之前的想法

这样既扩散了思路

同时又没有偏离讨论的论题

而是在不断的深化思考问题

**我在另一篇文章称之为思辨**

GRPO引起的思辨是我认为的DeepSeekR1的成功原因之一，也是其CoT异常冗长的原因。

通过不断的自我怀疑来推进思考

甚至自我怀疑到1+1=？的问题都要考虑很多

![](/assets/images/2025-05-30-DeepSeekR1的GRPO的loss实现问题-DeepSeekChat一加一问题例子.png)

**而且这种方式是无偏向的，对任何问题都能够反思。**

无偏向的能力提升，也许能是前往AGI的道路？

不过AGI实在太泛泛了，很难被定义，很难被讨论。

所以就说特定领域问题的RL训练，**进阶的核心，就是这种能够收敛到特定领域问题的发散能力**

**【收敛性的发散能力】**

当然，除了这种自我怀疑的CoT，也能有其他办法做到这种**收敛性的发散**。

而且不用CoT也能通过增加模型信息熵来扩大其发散路径。

不过**CoT有另一份好处，它更符合现有语料的习惯，更适配语言大模型。**

以此推展，非语言大模型，视觉大模型，说不定也有其他更适合的形式呢？

但也因为思辨CoT符合语料的特性（很多语料作者写文字就有这种自我怀疑的倾向），我也担心这样的RL，**究竟是真的在反思，还是在模仿反思**。

但我想这个是能够被验证的，不曾出现的题目，形式问题推导（比如WJU），以及创造全新语言等方法能够去验证这个。

或者再功利一点，模仿反思也是反思，好用就完事了。

就像我们的打字猴子，也许我们很难界定它是否真的思考，但我们可以期望它表现的足够思辨。

# 总结
强化学习RL是我特别喜欢的一个领域，

但越是喜欢，就得越得怀疑它，将它变成盲信可就不好了。

RL有用吗？

一定是有用的，很多成果和迹象都证明了其能力，而且RL本质有点实践检验的意味。

**而在RL过程中，能够起到突破作用的必定是随机的发散。**

我们没有无限的时间给猴子去打字。

要让RL在现实中有效，**需要的是收敛性的发散，既发散，又不脱离当前问题**。

**自我怀疑或者说是思辨可以做到这种收敛性的发散。**

而且其很适合概率模型。

无偏的思辨也许是通往AGI的一条路，但那个目标太过于虚幻了。

但用收敛性的发散，作为特定领域问题的指导，可能是很有效的一种方式。

一个全能的猴子不像现实的，

但一个会自己批判自己的猴子，好像还是能找到的哈哈。

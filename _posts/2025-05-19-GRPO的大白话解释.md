---
title: "为什么越'自我怀疑'的AI越聪明？DeepSeek R1的长思维链秘密"
date: 2025-05-19
categories: ["原理", "教程", "论文"]
tags: [llm, grpo, deepseek]
---


群组相对策略优化GRPO(Group Relative Policy Optimization)，通俗说就是用三个臭皮匠变成一个诸葛亮

**但怎么让三个臭皮匠变成诸葛亮，是个难题**

本文通过分析DeepSeek r1的群组相对优化策略，向你解释DeepSeek是如何做到的，并试图一窥人类思维的机制。

本文试图回答以下问题
1. DeepSeek r1是如何通过群组策略让模型变智慧
2. 为什么DeepSeek r1的思维链非常长？有什么意义？
3. 怎么样才算智慧的模型？智慧的核心是什么


先从论文公式入手

$$
\mathcal{J}_{\text{GRPO}}(\theta) = \mathbb{E}[q \sim P(Q),\, \{o_i\}_{i=1}^{G} \sim \pi_{\theta_{\text{old}}}(\mathcal{O}|q)] \\ 
\frac{1}{G} \sum_{i=1}^{G} \left( 
\min\left( 
\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)} A_i,\ 
\operatorname{clip}\left( 
\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)},\ 1 - \varepsilon,\ 1 + \varepsilon 
\right) A_i 

\right) 
- \beta \mathbb{D}_{\text{KL}}(\pi_{\theta} \| \pi_{\text{ref}}) 
\right) 
$$

不用被数学公式吓到，拆解一下就变得很清晰了

**首先我们要知道模型π是概率模型，它表达的是生成某个答案的概率。**

Q就是全部问题空间，q是某个具体问题。

E包着的是叫联合期望，是某个具体问题q与多个具体答案o的出现概率乘积
通俗来说就是越稀有的答案权重越低

举例来说小明做高考真题，有个考题出过但特别偏门，咱就不让小明花精力去学习它

**这个期望当题目q和具体答案o给定时是常数，我们可以忽略**
$$
\mathbb{E}[q \sim P(Q),\, \{o_i\}_{i=1}^{G} \sim \pi_{\theta_{\text{old}}}(\mathcal{O}|q)] 
$$

**DKL是散度，它的目标是让模型和参考模型相差不太大，它是一种强制约束，对所有学生一视同仁。我们现在可以忽略它**
$$
\mathbb{D}_{\mathrm{KL}}(\pi_{\theta} \| \pi_{\mathrm{ref}}) = 
\frac{\pi_{\mathrm{ref}}(o_i | q)}{\pi_{\theta}(o_i | q)} 
- \log \frac{\pi_{\mathrm{ref}}(o_i | q)}{\pi_{\theta}(o_i | q)} - 1
$$

**clip是截断函数，你看它第一个参数跟左边一样的。**
它就是避免里面的值太大或者太小，避免某一些特殊的学生答案造成巨大影响。我们先不考虑极端学生，因此可以忽略它
$$
\operatorname{clip}\left( 
\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)},\ 1 - \varepsilon,\ 1 + \varepsilon 
\right) A_i 
$$

**于是你看，这个公式就剩下这一小块了，也是强化学习中最核心的一部分**
$$
\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)}A_i 
$$

左边的部分是概率正则化，它除以旧概率以保证概率计算分数接近1这个值。

**其中右边Ai是分数，越高表示答案的正确率越高**
$$
A_i = \frac{r_i - \text{mean}(\{r_1, r_2, \cdots, r_G\})}{\text{std}(\{r_1, r_2, \cdots, r_G\})}
$$

分数很好理解，我们举例说明。

举例 小明、小王和小红三个同学一起答题。

小明答对了，小王和小红答错了。小明分就高

则新的学生会以小明为榜样

> 举例 <br>
> 提问 1 + 1 = ？ <br>
> 小王：3 <br>
> 小红：3 <br>
> 小明：2 <br>
> <br>
> 小明对了，新同学要以小明为榜样哦

**这就是公式里的Ai起的作用，以高分学生为榜样**

那么Ai左边那一坨πθ和πθold是什么呢？

πθ表示新的学生，就是要训练的对象

πθold是旧的学生的概率，可以理解为学生对这个答案的自信程度

**小明 小王 小红共用一个脑子πθold**

**脑子πθold是一个抽象概率，而小明 小王 小红是具体的人**

**πθold(oi|q)就是在给出问题q的情况下，小明，小王和小红给出具体答案o的概率。**

比如说，一道题答案有A B C D四个，脑子认为A的概率是90%，B的概率是10% C的概率0% D %0

虽然大部分学生都会选A，但保不齐有个学生选B，大概每10个学生里就会有一个选B

比如小明就选了B，那它对这个答案B的自信程度就是10%

然后小明答对了，新的学生就以小明为榜样

那这有什么用呢？小明只是蒙到的B啊，这具体要怎么做榜样呢？

**这就要引出思维链COT(Chain of Thought)的概念了**

依旧是小明 小王 小红做题

但不仅给出题目答案，还要给出过程草稿，也就是思维的中间过程

![](/assets/images/2025-05-19-GRPO的大白话解释-图中的部分就是思维链.png)


**所以πθold(o小明|q)不仅是小明对自己答案的自信程度，而且还是对自己的草稿过程的自信程度**

最后小明答案对了，小明成了榜样

**新的学生要学的不仅是答案，还是小明的解题过程，也就是学小明的草稿**

> 举例 <br>
> 提问 1 + 1 = ? <br>
> 
> 小王：这么简单，显然是3 <br>
> 小红：有三个符号，所以是3 <br>
> 小明：有三个符号，所以是，不对！这里有两个1，所以答案是2 <br>
> <br>
> 小明对了，新同学要以小明为榜样，还要多学习小明的思路哦


当问题过于简单或者困难，所有人得分都一样时，就没有任何学习余地了

> 举例 <br>
> 提问 1 + 1 = ? <br>
>  <br>
> 小王：这么简单，显然是3 <br>
> 小红：有三个符号，所以是3 <br>
> 小明：有三个符号，所以是，不对！这里有4个符号，所以答案是4
> <br>
> 所有人都错了，新同学们没法学啊 

**但更多的情况是，有得分，但不少策略得分相近的情况**

在正则化中，策略的概率是作为分母，在训练时作为常数项
$$
\frac{\pi_{\theta}(o_i|q)}{\pi_{\theta_{\text{old}}}(o_i|q)} 
$$


换句话说，概率越低，权重越高

那么这是个什么意思呢？

有两种情况，

1 是本身其概率就低，模型对自己的答案更加怀疑

2 是更可能的情况，输出文字量变多了。

**我们知道模型预测的是文字概率，而长文本则是概率乘积，也就是说文字量越长，概率越低。**

**于是，会倾向于COT更长且更不自信的输出**

> 举例 <br>
> 提问 1 + 1 = ？ <br>
> 
> 小王：这么简单，显然是3<br>
> 小红：这么简单，显然是2<br>
> 小明：虽然三个符号，所以是可能是3。但我发现这里有两个1，中间是+号需要累加，所以答案是2<br>
> <br>
> 小红和小王都对了，都是你们的榜样，但是小明思考的更长更纠结一点，所以你们要更向小明学习哦


**因此，这种策略会导向 模型变得更加思辨（怀疑自己），更加冗长（思考更多）**


当然我们可以增加对思考过程长度的计分，从而降低COT的长度，但这样会导向对过程的评判，走向对推理过程判断的思路

所以我们可以看到DeepSeek R1的输出相比GPT的reason模型会更加冗长

![](/assets/images/2025-05-19-GRPO的大白话解释-DeepSeek对比ChatGPT的思维链例子.png)
（上图测试并不严谨）


过去我一直认为COT是没有意义的。因为从概率角度上，模型只是概率的表达，COT仅仅增加了长度并没有改变概率本身。

上面的说法依旧是对的，在有限的问题空间下，我们显然可以把任意COT模型的概率完全转换为非COT模型的概率分布

**但，训练不一样，COT提升的是训练时的分辨率**

模型的训练是一个梯度下降的过程，需要以极小的步伐去接近最优
做过量化的都知道，无法训练一个int4的模型，比如你无法从整数空间获取x^2=2的数值解
![](/assets/images/2025-05-19-GRPO的大白话解释-梯度下降的示意图.png)

大白话来说

一个小孩写作业只写答案，老师只告诉他正确答案，这小孩参加高考，然后对不少
另一个小孩写作业需要写草稿，老师告诉他答案，也让他自己改草稿过程，这小孩参加高考，每道题都打草稿，做的慢一些，但也对不少

前者小孩是天才，是存在的，就像某些天才数学家的无需过程

后者是人才，数量没有天才小孩那么多，但好在更容易教出来

为什么更容易教出来？

因为能知道小孩思考的中间过程

**这就是COT的意义，提升了训练时的中间过程，让训练更有机会收敛**


当然这一切都是基于小孩本身能力的

如果小孩就是瞎写一通，那怎么教也没用

换算成模型，就依赖模型本身能力

如果模型的基础概率空间不够，那COT也只能增加有限的概率空间（理论上概率空间是相乘的，但是因为概率不是均等的，这个COT增加的概率空间会小很多）

如果题目过于困难，小孩无论怎么做都做不对，那也无法教会


上面的讨论都涉及【老师】

**如果老师不存在会怎么样呢？**

这就回到了GRPO，他通过群组学习，让即使老师不存在，也能进行过程学习
多个学生写出草稿与答案后，尽量学习得分高的学生的草稿，如果得分相同，尽量学习草稿长且思辨的学生的草稿

**这就是GRPO的隐藏价值观——追求思辨**

其表现就是DeepSeek r1论文提到的顿悟时刻——重新思考

**模型不停的否决自己，不停的重新思考**

从而变得更加思辨

![](/assets/images/2025-05-19-GRPO的大白话解释-啊哈时刻与重新思考.png)


不过思辨也可以换个差点的词——瞎想

**所以我们会见到有一些简单问题，DeepSeek不停瞎想，然后给出一个错误答案**
（比如经典问题strawberry有几个r）

![](/assets/images/2025-05-19-GRPO的大白话解释-猜灯谜的例子.png)
(我更喜欢这个猜灯谜的例子，图中模型已经找到了正确答案，但是依旧不停否定最后给出了错误答案，让我没能抢到红包)

我们必须再次提问，增加思辨能力真的可以提高模型智慧吗？

**从数学角度，提升思辨也就是提升了概率的不确定性，也就是熵增或者信息增。思辨的模型会更多去探索不同的选择，从而更容易泛化其模型能力。**

从训练角度，如果我们不是选择思辨的策略（概率低），而是选择自信的策略（概率高）。**在新一轮的运行中，自信的策略会被更容易被选择，从而收敛到一个局部空间上。**

比喻来说，小孩若是不进行思辨，他可能变成一个无情的背题机器，对给定的问题很有自信，但对新的问题很难举一反三。

但现实不如此，我们知道能背题的小孩往往也挺聪明。聪明小孩不想太多，但想的很巧妙。就像数学几何题，画了那条辅助线马上就能做，没画辅助线想破脑子也做不出。

让我们稍稍跳出模型

评价一小孩

一种小孩每次考试都打很多草稿然后写答案，另一种小孩凭直觉就能迅速写出答案

你觉得哪个小孩更聪明？

我的想法是无法判断

我认为不能神话COT与GRPO

对于知识与认识我们了解的甚少，增加一个人的思考时间并不见得让他变得更智慧，COT并不见得是未来的道路

DeepSeek r1论文里也重点放在了工程上，是让训练成本更低，还能去除评价模型。

而且我们有理由怀疑，模型在学习过程中不是增加了智慧，而是增加了对思辨文风的模仿。

**而且思辨对数学与推理问题有帮助，但对文学或者小说写作的范畴，想太多可能并无好处。**

有的朋友可能试过，DeepSeek r1模型写小说可能不如原本的chat模型
当然这也与文学类问题难以用客观分评判有关，而且未经训练有关。

## 总结
现在对于三个问题我们可以做出回答了

1. DeepSeek r1是如何通过群组策略让模型变智慧<br>
通过不停的做题，并以高分与思维不自信的学生答案为榜样

2. 为什么DeepSeek r1的思维链非常长？有什么意义？<br>
因为思维长的学生对自己的思维过程更不自信，这扩大了学生的解的思路，会考虑更多的方向
   
3. 怎么样才算智慧的模型？智慧的核心是什么<br>
追求的是正确率高且自我怀疑的模型，这里核心便是思辨。

总而言之，思辨是DeepSeek r1的目标
思维链增加的文本量，本质也是增大思辨的量

一个有意思的事，r1这个词，输入法打出来是【人】

DeepSeek r1似乎抓住了人类智慧的一个关键——思辨





Ref:
https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf










